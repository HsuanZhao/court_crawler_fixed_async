"""
ä¸Šæµ·å¸‚é«˜çº§äººæ°‘æ³•é™¢æ–‡ä¹¦æŠ“å–å·¥å…·ï¼ˆä¿®å¤ç¿»é¡µæ£€æµ‹é—®é¢˜ç‰ˆï¼‰
ä½¿ç”¨æ–¹æ³•ï¼špython sh_court_fixed_async_page.py
"""

import asyncio
import json
import random
import re
from datetime import datetime
from pathlib import Path
import pandas as pd
from playwright.async_api import async_playwright

class FixedAsyncCourtCrawler:
    def __init__(self, headless=False, max_cases=30, output_dir="æŠ“å–ç»“æœ"):
        self.headless = headless
        self.max_cases = max_cases
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.all_cases = []
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.json_file = self.output_dir / f"cases_{timestamp}.json"
        self.csv_file = self.output_dir / f"cases_{timestamp}.csv"
        
        self.stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'pages': 0,
            'start': datetime.now().isoformat()
        }
    
    async def random_delay(self, min_sec=1, max_sec=3):
        await asyncio.sleep(random.uniform(min_sec, max_sec))
    
    async def submit_search(self, page):
        """æäº¤æœç´¢è¡¨å•"""
        print("ğŸ” æäº¤æœç´¢è¡¨å•...")
        
        try:
            # ç­‰å¾…é¡µé¢åŠ è½½
            await page.wait_for_load_state('networkidle', timeout=15000)
            await self.random_delay(1, 2)
            
            # ç›´æ¥é€šè¿‡JavaScriptæäº¤
            submit_script = """
            () => {
                const forms = document.querySelectorAll('form');
                if (forms.length > 0) {
                    forms[0].submit();
                    console.log('è¡¨å•æäº¤æˆåŠŸ');
                    return true;
                }
                return false;
            }
            """
            
            result = await page.evaluate(submit_script)
            if result:
                print("âœ… è¡¨å•å·²æäº¤")
            
            # ç­‰å¾…ç»“æœåŠ è½½
            print("â³ ç­‰å¾…æœç´¢ç»“æœ...")
            await page.wait_for_timeout(5000)
            
            # æ£€æŸ¥æ˜¯å¦å‡ºç°æ–‡ä¹¦è¡Œ
            has_case_rows = await page.locator('tr[id^="tr"]').count() > 0
            if has_case_rows:
                print("âœ… æ£€æµ‹åˆ°æ–‡ä¹¦æ•°æ®è¡Œ")
                return True
            else:
                print("âš ï¸ æœªæ£€æµ‹åˆ°æ–‡ä¹¦è¡Œï¼Œç»§ç»­å°è¯•...")
                return True
                
        except Exception as e:
            print(f"âŒ è¡¨å•æäº¤å¤±è´¥: {e}")
            return False
    
    async def analyze_page_control(self, page):
        """åˆ†æåˆ†é¡µæ§ä»¶å¹¶æ‰“å°è¯¦ç»†ä¿¡æ¯"""
        try:
            print("ğŸ” åˆ†æåˆ†é¡µæ§ä»¶...")
            
            # ç­‰å¾…åˆ†é¡µæ§ä»¶åŠ è½½
            try:
                await page.wait_for_selector('div.meneame, .meneame, center#flws_list_content', timeout=5000)
            except:
                print("  âš ï¸ ç­‰å¾…åˆ†é¡µæ§ä»¶è¶…æ—¶")
            
            # è·å–åˆ†é¡µåŒºåŸŸçš„æ‰€æœ‰HTML
            page_control_script = """
            () => {
                const pageDiv = document.querySelector('div.meneame') || 
                               document.querySelector('.meneame') ||
                               document.querySelector('center#flws_list_content');
                
                if (!pageDiv) {
                    return {html: 'æœªæ‰¾åˆ°åˆ†é¡µæ§ä»¶', links: []};
                }
                
                const html = pageDiv.innerHTML;
                const links = [];
                
                // æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
                const allLinks = pageDiv.querySelectorAll('a');
                allLinks.forEach((link, index) => {
                    links.push({
                        index: index,
                        href: link.getAttribute('href') || '',
                        onclick: link.getAttribute('onclick') || '',
                        text: link.textContent || link.innerText || '',
                        outerHTML: link.outerHTML
                    });
                });
                
                // æŸ¥æ‰¾å½“å‰é¡µç 
                const currentSpan = pageDiv.querySelector('span.current');
                const currentPage = currentSpan ? (currentSpan.textContent || currentSpan.innerText) : '';
                
                return {
                    html: html,
                    links: links,
                    currentPage: currentPage,
                    hasCurrentSpan: !!currentSpan,
                    totalLinks: allLinks.length
                };
            }
            """
            
            result = await page.evaluate(page_control_script)
            
            print(f"  åˆ†é¡µæ§ä»¶HTMLï¼ˆå‰500å­—ç¬¦ï¼‰: {result['html'][:500]}...")
            print(f"  å½“å‰é¡µç : {result['currentPage']}")
            print(f"  æ˜¯å¦æœ‰current span: {result['hasCurrentSpan']}")
            print(f"  æ€»é“¾æ¥æ•°: {result['totalLinks']}")
            
            # æ‰“å°æ‰€æœ‰é“¾æ¥è¯¦ç»†ä¿¡æ¯
            print(f"  é“¾æ¥è¯¦ç»†ä¿¡æ¯:")
            for link in result['links']:
                print(f"    [{link['index']}] text='{link['text']}' onclick='{link['onclick']}' href='{link['href']}'")
            
            return result
            
        except Exception as e:
            print(f"âŒ åˆ†é¡µæ§ä»¶åˆ†æå¤±è´¥: {e}")
            return None
    
    async def check_and_go_next_page(self, page, current_page_num):
        """æ£€æŸ¥å¹¶è·³è½¬åˆ°ä¸‹ä¸€é¡µ"""
        try:
            print(f"ğŸ” å°è¯•ç¿»é¡µï¼Œå½“å‰åº”è¯¥æ˜¯ç¬¬{current_page_num}é¡µ")
            
            # å…ˆåˆ†æåˆ†é¡µæ§ä»¶
            page_info = await self.analyze_page_control(page)
            
            if not page_info:
                print("âŒ æ— æ³•è·å–åˆ†é¡µä¿¡æ¯")
                return False, current_page_num
            
            # æŸ¥æ‰¾ä¸‹ä¸€é¡µé“¾æ¥
            next_page_num = current_page_num + 1
            
            # æ–¹æ³•1ï¼šæŸ¥æ‰¾åŒ…å«ä¸‹ä¸€é¡µæ•°å­—çš„é“¾æ¥
            print(f"  æ–¹æ³•1: æŸ¥æ‰¾é¡µç  {next_page_num} çš„é“¾æ¥")
            for link in page_info['links']:
                # æ£€æŸ¥onclickä¸­çš„é¡µç 
                if link['onclick']:
                    # åŒ¹é… goPage æˆ– soPage
                    matches = re.findall(r'(?:goPage|soPage)\s*\(\s*[\'\"]?(\d+)[\'\"]?\s*\)', link['onclick'])
                    for match in matches:
                        if int(match) == next_page_num:
                            print(f"    âœ… æ‰¾åˆ°onclickç¿»é¡µé“¾æ¥: {link['onclick']}")
                            await page.evaluate(f"() => {{ {link['onclick']} }}")
                            await self.wait_for_page_load(page, next_page_num)
                            return True, next_page_num
                
                # æ£€æŸ¥hrefä¸­çš„é¡µç 
                if link['href'] and 'javascript:' in link['href']:
                    matches = re.findall(r'(?:goPage|soPage)\s*\(\s*[\'\"]?(\d+)[\'\"]?\s*\)', link['href'])
                    for match in matches:
                        if int(match) == next_page_num:
                            print(f"    âœ… æ‰¾åˆ°hrefç¿»é¡µé“¾æ¥: {link['href']}")
                            await page.click(f'a[href="{link["href"]}"]')
                            await self.wait_for_page_load(page, next_page_num)
                            return True, next_page_num
            
            # æ–¹æ³•2ï¼šæŸ¥æ‰¾"ä¸‹ä¸€é¡µ"æ–‡æœ¬çš„é“¾æ¥
            print(f"  æ–¹æ³•2: æŸ¥æ‰¾'ä¸‹ä¸€é¡µ'æ–‡æœ¬çš„é“¾æ¥")
            for link in page_info['links']:
                if link['text'] and ('ä¸‹ä¸€é¡µ' in link['text'] or 'next' in link['text'].lower()):
                    print(f"    âœ… æ‰¾åˆ°'ä¸‹ä¸€é¡µ'æ–‡æœ¬é“¾æ¥: {link['text']}")
                    
                    if link['onclick']:
                        await page.evaluate(f"() => {{ {link['onclick']} }}")
                    elif link['href']:
                        await page.click(f'a[href="{link["href"]}"]')
                    else:
                        # ç›´æ¥ç‚¹å‡»
                        await page.locator('a').nth(link['index']).click()
                    
                    await self.wait_for_page_load(page, next_page_num)
                    return True, next_page_num
            
            # æ–¹æ³•3ï¼šå°è¯•ç‚¹å‡»å½“å‰é¡µä¹‹åçš„ç¬¬ä¸€ä¸ªé“¾æ¥
            print(f"  æ–¹æ³•3: å°è¯•ç‚¹å‡»å½“å‰é¡µåçš„é“¾æ¥")
            if page_info['currentPage']:
                try:
                    current_page = int(page_info['currentPage'])
                    # æŸ¥æ‰¾æ¯”å½“å‰é¡µå¤§çš„ç¬¬ä¸€ä¸ªé“¾æ¥
                    for link in page_info['links']:
                        if link['onclick']:
                            matches = re.findall(r'(?:goPage|soPage)\s*\(\s*[\'\"]?(\d+)[\'\"]?\s*\)', link['onclick'])
                            for match in matches:
                                page_num = int(match)
                                if page_num > current_page:
                                    print(f"    âœ… æ‰¾åˆ°é¡µç  {page_num} çš„é“¾æ¥")
                                    await page.evaluate(f"() => {{ {link['onclick']} }}")
                                    await self.wait_for_page_load(page, page_num)
                                    return True, page_num
                except:
                    pass
            
            # æ–¹æ³•4ï¼šå¦‚æœåªæœ‰æ•°å­—é“¾æ¥ï¼Œå°è¯•ç‚¹å‡»æœ€åä¸€ä¸ªé“¾æ¥
            print(f"  æ–¹æ³•4: å°è¯•æœ€åä¸€ä¸ªæ•°å­—é“¾æ¥")
            if page_info['links']:
                last_link = page_info['links'][-1]
                if last_link['onclick']:
                    matches = re.findall(r'(?:goPage|soPage)\s*\(\s*[\'\"]?(\d+)[\'\"]?\s*\)', last_link['onclick'])
                    if matches:
                        last_page = int(matches[-1])
                        if last_page > current_page_num:
                            print(f"    âœ… ç‚¹å‡»æœ€åé¡µç é“¾æ¥: {last_page}")
                            await page.evaluate(f"() => {{ {last_link['onclick']} }}")
                            await self.wait_for_page_load(page, last_page)
                            return True, last_page
            
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„ç¿»é¡µæ–¹æ³•")
            return False, current_page_num
            
        except Exception as e:
            print(f"âŒ ç¿»é¡µå¤±è´¥: {e}")
            return False, current_page_num
    
    async def wait_for_page_load(self, page, page_num):
        """ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ"""
        print(f"â³ ç­‰å¾…ç¬¬{page_num}é¡µåŠ è½½...")
        
        # ç­‰å¾…ç½‘ç»œç©ºé—²
        try:
            await page.wait_for_load_state('networkidle', timeout=10000)
        except:
            print("  âš ï¸ ç­‰å¾…networkidleè¶…æ—¶")
        
        # ç­‰å¾…æ–‡ä¹¦è¡Œé‡æ–°å‡ºç°
        try:
            await page.wait_for_selector('tr[id^="tr"]', timeout=10000)
        except:
            print("  âš ï¸ ç­‰å¾…æ–‡ä¹¦è¡Œè¶…æ—¶")
        
        # é¢å¤–ç­‰å¾…æ—¶é—´
        await self.random_delay(2, 3)
        
        # æ£€æŸ¥æ–‡ä¹¦æ•°é‡
        rows_count = await page.locator('tr[id^="tr"]').count()
        print(f"âœ… ç¬¬{page_num}é¡µåŠ è½½å®Œæˆï¼Œæœ‰ {rows_count} ä¸ªæ–‡ä¹¦")
        
        return True
    
    async def extract_case_data(self, page, current_page=1):
        """æå–æ–‡ä¹¦æ•°æ®"""
        print(f"ğŸ“Š æå–ç¬¬{current_page}é¡µæ–‡ä¹¦æ•°æ®...")
        
        cases = []
        try:
            # ç­‰å¾…æ–‡ä¹¦è¡Œå‡ºç°
            await page.wait_for_selector('tr[id^="tr"]', timeout=15000)
            
            # æ‰¾åˆ°æ‰€æœ‰æ–‡ä¹¦è¡Œ
            case_rows = await page.locator('tr[id^="tr"]').all()
            print(f"æ‰¾åˆ° {len(case_rows)} ä¸ªæ–‡ä¹¦è¡Œ")
            
            for i, row in enumerate(case_rows):
                try:
                    # è·å–è¡Œå±æ€§
                    row_id = await row.get_attribute('id') or f"tr_{current_page}_{i}"
                    onclick_attr = await row.get_attribute('onclick') or ""
                    
                    # æå–åŠ å¯†å‚æ•°
                    detail_param = ""
                    if onclick_attr:
                        match = re.search(r"showone\('([^']+)'\)", onclick_attr)
                        if match:
                            detail_param = match.group(1)
                    
                    # æå–æ‰€æœ‰å•å…ƒæ ¼
                    cells = await row.locator('td').all()
                    
                    if len(cells) >= 7:
                        # åˆ†åˆ«è·å–æ¯ä¸ªå•å…ƒæ ¼çš„æ–‡æœ¬
                        case_number = await cells[0].inner_text()
                        title = await cells[1].inner_text()
                        doc_type = await cells[2].inner_text()
                        
                        case_reason_text = await cells[3].inner_text()
                        case_reason = case_reason_text.replace('&nbsp;', '').strip()
                        
                        department_text = await cells[4].inner_text()
                        department = department_text.replace('&nbsp;', '').strip()
                        
                        level_text = await cells[5].inner_text()
                        level = level_text.replace('&nbsp;', '').strip()
                        
                        close_date = await cells[6].inner_text()
                        
                        case_data = {
                            'row_id': row_id,
                            'case_number': case_number.strip(),
                            'title': title.strip(),
                            'doc_type': doc_type.strip(),
                            'case_reason': case_reason,
                            'department': department,
                            'level': level,
                            'close_date': close_date.strip(),
                            'detail_param': detail_param,
                            'row_index': i,
                            'page_number': current_page
                        }
                        
                        # æ„å»ºè¯¦æƒ…é¡µURL
                        if detail_param:
                            base_url = "https://www.hshfy.sh.cn/shfy/web/flws_view.jsp"
                            case_data['detail_url'] = f"{base_url}?pa={detail_param}"
                        else:
                            case_data['detail_url'] = ""
                        
                        cases.append(case_data)
                        print(f"  å·²æå–: {case_data['case_number']} (ç¬¬{current_page}é¡µ)")
                        
                except Exception as e:
                    print(f"  ç¬¬{current_page}é¡µç¬¬{i}è¡Œæå–å¤±è´¥: {str(e)[:100]}")
                    continue
            
            self.stats['total'] += len(case_rows)
            print(f"âœ… æˆåŠŸæå– {len(cases)} ä¸ªæ–‡ä¹¦ (ç¬¬{current_page}é¡µ)")
            return cases
            
        except Exception as e:
            print(f"âŒ ç¬¬{current_page}é¡µæ•°æ®æå–å¤±è´¥: {e}")
            await page.screenshot(path=self.output_dir / f'extract_error_page{current_page}.png')
            return cases
    
    async def crawl_detail_page(self, context, case_data, main_page):
        """æŠ“å–è¯¦æƒ…é¡µå†…å®¹"""
        print(f"ğŸ“„ æ‰“å¼€è¯¦æƒ…é¡µ: {case_data['case_number']} (ç¬¬{case_data['page_number']}é¡µ)")
        
        if not case_data.get('detail_url'):
            print("  âš ï¸ æ— è¯¦æƒ…é“¾æ¥ï¼Œè·³è¿‡")
            return None
        
        detail_page = None
        try:
            # ç›‘å¬æ–°é¡µé¢æ‰“å¼€
            async with context.expect_page() as new_page_info:
                # ç‚¹å‡»å¯¹åº”çš„è¡Œ
                try:
                    row_selector = f'tr[id="{case_data["row_id"]}"]'
                    if await main_page.locator(row_selector).count() > 0:
                        await main_page.click(row_selector)
                        print(f"  ç‚¹å‡»è¡Œ: {case_data['row_id']}")
                    else:
                        # å¤‡é€‰ï¼šé€šè¿‡æ¡ˆå·æŸ¥æ‰¾
                        case_number_text = case_data['case_number'].replace('(', '\\(').replace(')', '\\)')
                        text_selector = f'text="{case_number_text}"'
                        if await main_page.locator(text_selector).count() > 0:
                            await main_page.click(text_selector)
                            print(f"  ç‚¹å‡»æ¡ˆå·æ–‡æœ¬: {case_data['case_number']}")
                except Exception as e:
                    print(f"  ç‚¹å‡»å¤±è´¥: {e}")
                    # ç›´æ¥è®¿é—®URL
                    detail_page = await context.new_page()
                    await detail_page.goto(case_data['detail_url'], timeout=30000)
            
            # è·å–æ–°é¡µé¢
            if not detail_page:
                detail_page = await new_page_info.value
            
            # ç­‰å¾…è¯¦æƒ…é¡µåŠ è½½
            await detail_page.wait_for_load_state('networkidle', timeout=15000)
            await self.random_delay(0.5, 1.5)
            
            # æå–è¯¦æƒ…å†…å®¹
            detail_content = await self.extract_detail_content(detail_page)
            
            # åˆå¹¶æ•°æ®
            full_data = {**case_data, **detail_content}
            
            self.stats['success'] += 1
            print(f"âœ… è¯¦æƒ…é¡µæŠ“å–æˆåŠŸ (ç¬¬{case_data['page_number']}é¡µ)")
            return full_data
            
        except Exception as e:
            print(f"âŒ è¯¦æƒ…é¡µå¤±è´¥: {str(e)[:100]}")
            self.stats['failed'] += 1
            return None
        finally:
            if detail_page:
                await detail_page.close()
    
    async def extract_detail_content(self, page):
        """æå–è¯¦æƒ…é¡µå†…å®¹"""
        try:
            # ç­‰å¾…å†…å®¹åŠ è½½
            await page.wait_for_timeout(2000)
            
            # è·å–é¡µé¢å†…å®¹
            content = await page.content()
            
            # ç®€å•æå–æ–‡æœ¬
            text = await page.locator('body').inner_text()
            cleaned_text = ' '.join(text.split())  # åˆå¹¶å¤šä½™ç©ºæ ¼
            
            return {
                'detail_text': cleaned_text[:5000] + '...' if len(cleaned_text) > 5000 else cleaned_text,
                'detail_url': page.url,
                'detail_fetched_at': datetime.now().isoformat(),
                'content_length': len(content)
            }
        except Exception as e:
            print(f"  è¯¦æƒ…å†…å®¹æå–å¤±è´¥: {e}")
            return {}
    
    async def save_data(self):
        """ä¿å­˜æ•°æ®"""
        if not self.all_cases:
            print("âš ï¸ æ— æ•°æ®å¯ä¿å­˜")
            return
        
        print("ğŸ’¾ ä¿å­˜æ•°æ®...")
        
        try:
            # ä¿å­˜JSON
            with open(self.json_file, 'w', encoding='utf-8') as f:
                json.dump(self.all_cases, f, ensure_ascii=False, indent=2)
            print(f"   JSON: {self.json_file}")
            
            # ä¿å­˜CSV
            df = pd.DataFrame(self.all_cases)
            df.to_csv(self.csv_file, index=False, encoding='utf-8-sig')
            print(f"   CSV: {self.csv_file}")
            
            # ä¿å­˜ç®€ç‰ˆCSVï¼ˆä¸å«é•¿æ–‡æœ¬ï¼‰
            if 'detail_text' in df.columns:
                simple_df = df.drop(columns=['detail_text'])
                simple_file = self.csv_file.with_name(f"ç®€ç‰ˆ_{self.csv_file.name}")
                simple_df.to_csv(simple_file, index=False, encoding='utf-8-sig')
                print(f"   ç®€ç‰ˆCSV: {simple_file}")
                
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
    
    async def run(self, start_url):
        """ä¸»è¿è¡Œæµç¨‹"""
        print("=" * 50)
        print("ä¸Šæµ·å¸‚é«˜çº§äººæ°‘æ³•é™¢æ–‡ä¹¦æŠ“å–ï¼ˆä¿®å¤ç¿»é¡µæ£€æµ‹ç‰ˆï¼‰")
        print("=" * 50)
        
        playwright = None
        browser = None
        
        try:
            # å¯åŠ¨æµè§ˆå™¨
            playwright = await async_playwright().start()
            browser = await playwright.chromium.launch(
                headless=self.headless,
                args=['--start-maximized']
            )
            context = await browser.new_context(
                viewport={'width': 1200, 'height': 800}
            )
            
            # æ‰“å¼€é¡µé¢
            page = await context.new_page()
            print(f"ğŸŒ è®¿é—®: {start_url}")
            await page.goto(start_url, timeout=30000)
            
            # æäº¤æœç´¢
            if not await self.submit_search(page):
                print("âŒ æœç´¢å¤±è´¥ï¼Œç¨‹åºç»“æŸ")
                return
            
            current_page = 1
            total_processed = 0
            
            while total_processed < self.max_cases:
                print(f"\nğŸ“„ å¤„ç†ç¬¬ {current_page} é¡µ")
                print(f"å½“å‰ç´¯è®¡å¤„ç†: {total_processed}/{self.max_cases}")
                
                # ç­‰å¾…é¡µé¢ç¨³å®š
                if current_page > 1:
                    print("ğŸ”„ ç­‰å¾…ç¿»é¡µåé¡µé¢ç¨³å®š...")
                    await self.random_delay(3, 4)
                
                # æå–å½“å‰é¡µæ–‡ä¹¦
                cases = await self.extract_case_data(page, current_page)
                
                if not cases:
                    print("âš ï¸ æœªæå–åˆ°æ–‡ä¹¦æ•°æ®")
                    break
                
                # è®¡ç®—æœ¬é¡µéœ€è¦å¤„ç†å¤šå°‘æ–‡ä¹¦
                remaining = self.max_cases - total_processed
                cases_to_process = cases[:remaining]
                
                print(f"ğŸ“Š æœ¬é¡µå¤„ç† {len(cases_to_process)} ä¸ªæ–‡ä¹¦ (å‰©ä½™éœ€æ±‚: {remaining})")
                
                # æŠ“å–è¯¦æƒ…é¡µ
                for i, case in enumerate(cases_to_process):
                    print(f"\n[{total_processed + i + 1}/{self.max_cases}] {case['case_number']} (ç¬¬{current_page}é¡µ)")
                    
                    detail_data = await self.crawl_detail_page(context, case, page)
                    if detail_data:
                        self.all_cases.append(detail_data)
                        total_processed += 1
                        print(f"  å·²ä¿å­˜åˆ°åˆ—è¡¨ (ç´¯è®¡: {total_processed}/{self.max_cases})")
                    
                    # æ¯æŠ“å–2ä¸ªå°±ä¿å­˜ä¸€æ¬¡ï¼ˆé¿å…ä¸¢å¤±æ•°æ®ï¼‰
                    if (total_processed % 2 == 0) and total_processed > 0:
                        await self.save_data()
                    
                    # å»¶è¿Ÿï¼ˆé¿å…è¯·æ±‚è¿‡å¿«ï¼‰
                    if total_processed < self.max_cases:
                        delay = random.uniform(2, 4)
                        print(f"  ç­‰å¾… {delay:.1f}ç§’...")
                        await asyncio.sleep(delay)
                
                # æ›´æ–°è¿›åº¦
                self.stats['pages'] = current_page
                
                # æ£€æŸ¥æ˜¯å¦è¿˜éœ€è¦ç»§ç»­ç¿»é¡µ
                if total_processed >= self.max_cases:
                    print(f"âœ… å·²è¾¾åˆ°ç›®æ ‡æ•°é‡ {self.max_cases}")
                    break
                
                # å°è¯•ç¿»é¡µ
                print(f"\nğŸ”„ å°è¯•ç¿»é¡µåˆ°ç¬¬{current_page + 1}é¡µ...")
                success, new_page = await self.check_and_go_next_page(page, current_page)
                
                if success:
                    current_page = new_page
                    print(f"âœ… æˆåŠŸç¿»é¡µåˆ°ç¬¬{current_page}é¡µ")
                else:
                    print("âŒ ç¿»é¡µå¤±è´¥ï¼Œåœæ­¢æŠ“å–")
                    break
            
            # æœ€ç»ˆä¿å­˜
            await self.save_data()
            
            # ç»Ÿè®¡ä¿¡æ¯
            self.stats['end'] = datetime.now().isoformat()
            start = datetime.fromisoformat(self.stats['start'])
            end = datetime.fromisoformat(self.stats['end'])
            duration = (end - start).total_seconds()
            
            print("\n" + "=" * 50)
            print("âœ… æŠ“å–å®Œæˆï¼")
            print(f"   å‘ç°æ–‡ä¹¦æ€»æ•°: {self.stats['total']}")
            print(f"   å¤„ç†é¡µæ•°: {self.stats['pages']}")
            print(f"   æˆåŠŸæŠ“å–: {self.stats['success']}")
            print(f"   å¤±è´¥: {self.stats['failed']}")
            print(f"   ç›®æ ‡æ•°é‡: {self.max_cases}")
            print(f"   å®é™…æŠ“å–: {len(self.all_cases)}")
            print(f"   è€—æ—¶: {duration:.1f}ç§’")
            print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
            print("=" * 50)
            
        except Exception as e:
            print(f"\nâŒ ç¨‹åºè¿è¡Œå¼‚å¸¸: {str(e)[:200]}")
            import traceback
            traceback.print_exc()
        finally:
            # æ¸…ç†èµ„æº
            if browser:
                await browser.close()
            if playwright:
                await playwright.stop()
            
            # æœ€åä¿å­˜ä¸€æ¬¡
            if self.all_cases:
                await self.save_data()

async def main():
    """ä¸»å‡½æ•°"""
    config = {
        'start_url': 'https://www.hshfy.sh.cn/shfy/gweb2017/flws_list_new.jsp?ajlb=aYWpsYj3QzMrCz',
        'headless': False,  # è°ƒè¯•æ—¶è®¾ä¸ºFalse
        'max_cases': 30,    # æµ‹è¯•ç”¨30ä¸ªï¼Œä¼šè‡ªåŠ¨ç¿»é¡µ
        'output_dir': 'æœ€ç»ˆæŠ“å–æµ‹è¯•'
    }
    
    print("é…ç½®:")
    for k, v in config.items():
        print(f"  {k}: {v}")
    
    crawler = FixedAsyncCourtCrawler(
        headless=config['headless'],
        max_cases=config['max_cases'],
        output_dir=config['output_dir']
    )
    
    await crawler.run(config['start_url'])

if __name__ == "__main__":
    asyncio.run(main())