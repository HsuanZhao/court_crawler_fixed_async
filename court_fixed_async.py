"""
æŸå¸‚é«˜çº§äººæ°‘æ³•é™¢æ–‡ä¹¦æŠ“å–å·¥å…·ï¼ˆä¿®å¤å¼‚æ­¥é”™è¯¯ç‰ˆï¼‰
ä½¿ç”¨æ–¹æ³•ï¼špython court_fixed_async.py
"""

import asyncio
import json
import random
import re
from datetime import datetime
from pathlib import Path
import pandas as pd
from playwright.async_api import async_playwright

class FixedAsyncCourtCrawler:
    def __init__(self, headless=False, max_cases=3, output_dir="æŠ“å–ç»“æœ"):
        self.headless = headless
        self.max_cases = max_cases
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.all_cases = []
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.json_file = self.output_dir / f"cases_{timestamp}.json"
        self.csv_file = self.output_dir / f"cases_{timestamp}.csv"
        
        self.stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'start': datetime.now().isoformat()
        }
    
    async def random_delay(self, min_sec=1, max_sec=3):
        await asyncio.sleep(random.uniform(min_sec, max_sec))
    
    async def submit_search(self, page):
        """æäº¤æœç´¢è¡¨å•"""
        print("ğŸ” æäº¤æœç´¢è¡¨å•...")
        
        try:
            # ç­‰å¾…é¡µé¢åŠ è½½
            await page.wait_for_load_state('networkidle', timeout=15000)
            await self.random_delay(1, 2)
            
            # ç›´æ¥é€šè¿‡JavaScriptæäº¤
            submit_script = """
            () => {
                const forms = document.querySelectorAll('form');
                if (forms.length > 0) {
                    forms[0].submit();
                    console.log('è¡¨å•æäº¤æˆåŠŸ');
                    return true;
                }
                return false;
            }
            """
            
            result = await page.evaluate(submit_script)
            if result:
                print("âœ… è¡¨å•å·²æäº¤")
            
            # ç­‰å¾…ç»“æœåŠ è½½
            print("â³ ç­‰å¾…æœç´¢ç»“æœ...")
            await page.wait_for_timeout(5000)
            
            # æ£€æŸ¥æ˜¯å¦å‡ºç°æ–‡ä¹¦è¡Œ
            has_case_rows = await page.locator('tr[id^="tr"]').count() > 0
            if has_case_rows:
                print("âœ… æ£€æµ‹åˆ°æ–‡ä¹¦æ•°æ®è¡Œ")
                return True
            else:
                print("âš ï¸ æœªæ£€æµ‹åˆ°æ–‡ä¹¦è¡Œï¼Œç»§ç»­å°è¯•...")
                return True
                
        except Exception as e:
            print(f"âŒ è¡¨å•æäº¤å¤±è´¥: {e}")
            return False
    
    async def extract_case_data(self, page):
        """æå–æ–‡ä¹¦æ•°æ® - ä¿®å¤å¼‚æ­¥è°ƒç”¨"""
        print("ğŸ“Š æå–æ–‡ä¹¦æ•°æ®...")
        
        cases = []
        try:
            # ç­‰å¾…æ–‡ä¹¦è¡Œå‡ºç°
            await page.wait_for_selector('tr[id^="tr"]', timeout=10000)
            
            # æ‰¾åˆ°æ‰€æœ‰æ–‡ä¹¦è¡Œ
            case_rows = await page.locator('tr[id^="tr"]').all()
            print(f"æ‰¾åˆ° {len(case_rows)} ä¸ªæ–‡ä¹¦è¡Œ")
            
            # é™åˆ¶å¤„ç†æ•°é‡
            rows_to_process = case_rows[:self.max_cases]
            
            for i, row in enumerate(rows_to_process):
                try:
                    # è·å–è¡Œå±æ€§
                    row_id = await row.get_attribute('id') or f"tr{i}"
                    onclick_attr = await row.get_attribute('onclick') or ""
                    
                    # æå–åŠ å¯†å‚æ•°
                    detail_param = ""
                    if onclick_attr:
                        match = re.search(r"showone\('([^']+)'\)", onclick_attr)
                        if match:
                            detail_param = match.group(1)
                    
                    # æå–æ‰€æœ‰å•å…ƒæ ¼ - ä¿®å¤ï¼šç¡®ä¿æ¯ä¸ªinner_text()éƒ½ä½¿ç”¨await
                    cells = await row.locator('td').all()
                    
                    if len(cells) >= 7:
                        # åˆ†åˆ«è·å–æ¯ä¸ªå•å…ƒæ ¼çš„æ–‡æœ¬
                        case_number = await cells[0].inner_text()
                        title = await cells[1].inner_text()
                        doc_type = await cells[2].inner_text()
                        
                        # ä¿®å¤ï¼šawaitåå†è°ƒç”¨å­—ç¬¦ä¸²æ–¹æ³•
                        case_reason_text = await cells[3].inner_text()
                        case_reason = case_reason_text.replace('&nbsp;', '').strip()
                        
                        department_text = await cells[4].inner_text()
                        department = department_text.replace('&nbsp;', '').strip()
                        
                        level_text = await cells[5].inner_text()
                        level = level_text.replace('&nbsp;', '').strip()
                        
                        close_date = await cells[6].inner_text()
                        
                        case_data = {
                            'row_id': row_id,
                            'case_number': case_number.strip(),
                            'title': title.strip(),
                            'doc_type': doc_type.strip(),
                            'case_reason': case_reason,
                            'department': department,
                            'level': level,
                            'close_date': close_date.strip(),
                            'detail_param': detail_param,
                            'row_index': i
                        }
                        
                        # æ„å»ºè¯¦æƒ…é¡µURL
                        if detail_param:
                            base_url = "https://www.XXXXX.XX.cn/XXXX/web/flws_view.jsp" #æ³¨æ„è¦æ›¿æ¢ç½‘å€
                            case_data['detail_url'] = f"{base_url}?pa={detail_param}"
                        else:
                            case_data['detail_url'] = ""
                        
                        cases.append(case_data)
                        print(f"  å·²æå–: {case_data['case_number']}")
                        
                except Exception as e:
                    print(f"  ç¬¬{i}è¡Œæå–å¤±è´¥: {str(e)[:100]}")
                    continue
            
            self.stats['total'] = len(case_rows)
            print(f"âœ… æˆåŠŸæå– {len(cases)} ä¸ªæ–‡ä¹¦")
            return cases
            
        except Exception as e:
            print(f"âŒ æ•°æ®æå–å¤±è´¥: {e}")
            await page.screenshot(path=self.output_dir / 'extract_error.png')
            return cases
    
    async def crawl_detail_page(self, context, case_data, main_page):
        """æŠ“å–è¯¦æƒ…é¡µå†…å®¹"""
        print(f"ğŸ“„ æ‰“å¼€è¯¦æƒ…é¡µ: {case_data['case_number']}")
        
        if not case_data.get('detail_url'):
            print("  âš ï¸ æ— è¯¦æƒ…é“¾æ¥ï¼Œè·³è¿‡")
            return None
        
        detail_page = None
        try:
            # ç›‘å¬æ–°é¡µé¢æ‰“å¼€
            async with context.expect_page() as new_page_info:
                # ç‚¹å‡»å¯¹åº”çš„è¡Œ
                try:
                    row_selector = f'tr[id="{case_data["row_id"]}"]'
                    if await main_page.locator(row_selector).count() > 0:
                        await main_page.click(row_selector)
                        print(f"  ç‚¹å‡»è¡Œ: {case_data['row_id']}")
                    else:
                        # å¤‡é€‰ï¼šé€šè¿‡æ¡ˆå·æŸ¥æ‰¾
                        case_number_text = case_data['case_number'].replace('(', '\\(').replace(')', '\\)')
                        text_selector = f'text="{case_number_text}"'
                        if await main_page.locator(text_selector).count() > 0:
                            await main_page.click(text_selector)
                            print(f"  ç‚¹å‡»æ¡ˆå·æ–‡æœ¬: {case_data['case_number']}")
                except Exception as e:
                    print(f"  ç‚¹å‡»å¤±è´¥: {e}")
                    # ç›´æ¥è®¿é—®URL
                    detail_page = await context.new_page()
                    await detail_page.goto(case_data['detail_url'], timeout=30000)
            
            # è·å–æ–°é¡µé¢
            if not detail_page:
                detail_page = await new_page_info.value
            
            # ç­‰å¾…è¯¦æƒ…é¡µåŠ è½½
            await detail_page.wait_for_load_state('networkidle', timeout=15000)
            await self.random_delay(0.5, 1.5)
            
            # æå–è¯¦æƒ…å†…å®¹
            detail_content = await self.extract_detail_content(detail_page)
            
            # åˆå¹¶æ•°æ®
            full_data = {**case_data, **detail_content}
            
            self.stats['success'] += 1
            print(f"âœ… è¯¦æƒ…é¡µæŠ“å–æˆåŠŸ")
            return full_data
            
        except Exception as e:
            print(f"âŒ è¯¦æƒ…é¡µå¤±è´¥: {str(e)[:100]}")
            self.stats['failed'] += 1
            return None
        finally:
            if detail_page:
                await detail_page.close()
    
    async def extract_detail_content(self, page):
        """æå–è¯¦æƒ…é¡µå†…å®¹"""
        try:
            # ç­‰å¾…å†…å®¹åŠ è½½
            await page.wait_for_timeout(2000)
            
            # è·å–é¡µé¢å†…å®¹
            content = await page.content()
            
            # ç®€å•æå–æ–‡æœ¬
            text = await page.locator('body').inner_text()
            cleaned_text = ' '.join(text.split())  # åˆå¹¶å¤šä½™ç©ºæ ¼
            
            return {
                'detail_text': cleaned_text[:5000] + '...' if len(cleaned_text) > 5000 else cleaned_text,
                'detail_url': page.url,
                'detail_fetched_at': datetime.now().isoformat(),
                'content_length': len(content)
            }
        except Exception as e:
            print(f"  è¯¦æƒ…å†…å®¹æå–å¤±è´¥: {e}")
            return {}
    
    async def save_data(self):
        """ä¿å­˜æ•°æ®"""
        if not self.all_cases:
            print("âš ï¸ æ— æ•°æ®å¯ä¿å­˜")
            return
        
        print("ğŸ’¾ ä¿å­˜æ•°æ®...")
        
        try:
            # ä¿å­˜JSON
            with open(self.json_file, 'w', encoding='utf-8') as f:
                json.dump(self.all_cases, f, ensure_ascii=False, indent=2)
            print(f"   JSON: {self.json_file}")
            
            # ä¿å­˜CSV
            df = pd.DataFrame(self.all_cases)
            df.to_csv(self.csv_file, index=False, encoding='utf-8-sig')
            print(f"   CSV: {self.csv_file}")
            
            # ä¿å­˜ç®€ç‰ˆCSVï¼ˆä¸å«é•¿æ–‡æœ¬ï¼‰
            if 'detail_text' in df.columns:
                simple_df = df.drop(columns=['detail_text'])
                simple_file = self.csv_file.with_name(f"ç®€ç‰ˆ_{self.csv_file.name}")
                simple_df.to_csv(simple_file, index=False, encoding='utf-8-sig')
                print(f"   ç®€ç‰ˆCSV: {simple_file}")
                
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
    
    async def run(self, start_url):
        """ä¸»è¿è¡Œæµç¨‹"""
        print("=" * 50)
        print("æŸå¸‚é«˜çº§äººæ°‘æ³•é™¢æ–‡ä¹¦æŠ“å–ï¼ˆä¿®å¤å¼‚æ­¥ç‰ˆï¼‰")
        print("=" * 50)
        
        playwright = None
        browser = None
        
        try:
            # å¯åŠ¨æµè§ˆå™¨
            playwright = await async_playwright().start()
            browser = await playwright.chromium.launch(
                headless=self.headless,
                args=['--start-maximized']
            )
            context = await browser.new_context(
                viewport={'width': 1200, 'height': 800}
            )
            
            # æ‰“å¼€é¡µé¢
            page = await context.new_page()
            print(f"ğŸŒ è®¿é—®: {start_url}")
            await page.goto(start_url, timeout=30000)
            
            # æäº¤æœç´¢
            if not await self.submit_search(page):
                print("âŒ æœç´¢å¤±è´¥ï¼Œç¨‹åºç»“æŸ")
                return
            
            # æå–åˆ—è¡¨
            print("\nğŸ“‹ æå–æ–‡ä¹¦åˆ—è¡¨...")
            await self.random_delay(2, 3)
            cases = await self.extract_case_data(page)
            
            if not cases:
                print("âš ï¸ æœªæå–åˆ°æ–‡ä¹¦æ•°æ®")
                # ä¿å­˜å½“å‰é¡µé¢ä¾›è°ƒè¯•
                await page.screenshot(path=self.output_dir / 'no_cases_debug.png')
                return
            
            print(f"ğŸ“Š æ‰¾åˆ° {len(cases)} ä¸ªæ–‡ä¹¦ï¼Œå¼€å§‹æŠ“å–è¯¦æƒ…...")
            
            # æŠ“å–è¯¦æƒ…é¡µ
            for i, case in enumerate(cases):
                print(f"\n[{i+1}/{len(cases)}] {case['case_number']}")
                
                detail_data = await self.crawl_detail_page(context, case, page)
                if detail_data:
                    self.all_cases.append(detail_data)
                    print(f"  å·²ä¿å­˜åˆ°åˆ—è¡¨")
                
                # æ¯æŠ“å–1ä¸ªå°±ä¿å­˜ä¸€æ¬¡ï¼ˆé¿å…ä¸¢å¤±æ•°æ®ï¼‰
                if (i + 1) % 1 == 0:
                    await self.save_data()
                
                # å»¶è¿Ÿï¼ˆé¿å…è¯·æ±‚è¿‡å¿«ï¼‰
                if i < len(cases) - 1:
                    delay = random.uniform(2, 4)
                    print(f"  ç­‰å¾… {delay:.1f}ç§’...")
                    await asyncio.sleep(delay)
            
            # æœ€ç»ˆä¿å­˜
            await self.save_data()
            
            # ç»Ÿè®¡ä¿¡æ¯
            self.stats['end'] = datetime.now().isoformat()
            start = datetime.fromisoformat(self.stats['start'])
            end = datetime.fromisoformat(self.stats['end'])
            duration = (end - start).total_seconds()
            
            print("\n" + "=" * 50)
            print("âœ… æŠ“å–å®Œæˆï¼")
            print(f"   å‘ç°æ–‡ä¹¦: {self.stats['total']}")
            print(f"   æˆåŠŸæŠ“å–: {self.stats['success']}")
            print(f"   å¤±è´¥: {self.stats['failed']}")
            print(f"   è€—æ—¶: {duration:.1f}ç§’")
            print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
            print("=" * 50)
            
        except Exception as e:
            print(f"\nâŒ ç¨‹åºè¿è¡Œå¼‚å¸¸: {str(e)[:200]}")
            import traceback
            traceback.print_exc()
        finally:
            # æ¸…ç†èµ„æº
            if browser:
                await browser.close()
            if playwright:
                await playwright.stop()
            
            # æœ€åä¿å­˜ä¸€æ¬¡
            if self.all_cases:
                await self.save_data()

async def main():
    """ä¸»å‡½æ•°"""
    config = {
        'start_url': 'https://www.hshfy.sh.cn/shfy/gweb2017/flws_list_new.jsp?ajlb=aYWpsYj3QzMrCz',
        'headless': False,  # è°ƒè¯•æ—¶è®¾ä¸ºFalse
        'max_cases': 9,     # æµ‹è¯•ç”¨9ä¸ª
        'output_dir': 'æœ€ç»ˆæŠ“å–æµ‹è¯•'
    }
    
    print("é…ç½®:")
    for k, v in config.items():
        print(f"  {k}: {v}")
    
    crawler = FixedAsyncCourtCrawler(
        headless=config['headless'],
        max_cases=config['max_cases'],
        output_dir=config['output_dir']
    )
    
    await crawler.run(config['start_url'])

if __name__ == "__main__":

    asyncio.run(main())

